## Task 1
```text
❯ helm install --dry-run --debug helm-hooks python-helm
install.go:218: [debug] Original chart version: ""
install.go:235: [debug] CHART PATH: /Users/romanmolochkov/Developer/PycharmProjects/S24-DevOps-labs/k8s/python-helm

NAME: helm-hooks
LAST DEPLOYED: Sun Apr 28 18:58:55 2024
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: Always
  repository: rmoll/moscow_time_web_app
  tag: latest
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: false
  hosts:
  - host: chart-example.local
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
livenessProbe:
  httpGet:
    path: /
    port: http
nameOverride: ""
nodeSelector: {}
podAnnotations:
  vault.hashicorp.com/agent-inject: "true"
  vault.hashicorp.com/agent-inject-secret-config.txt: internal/data/data/config
  vault.hashicorp.com/role: internal-app
podLabels: {}
podManagementPolicy: Parallel
podSecurityContext: {}
readinessProbe:
  httpGet:
    path: /
    port: http
replicaCount: 3
resources:
  limits:
    cpu: 100m
    memory: 128Mi
  requests:
    cpu: 100m
    memory: 128Mi
securityContext: {}
service:
  port: 8080
  type: NodePort
serviceAccount:
  annotations: {}
  automount: true
  create: false
  name: internal-app
storage:
  size: 1Gi
tolerations: []
volumeMounts:
- mountPath: /app/config.json
  name: config-volume
  subPath: config.json
volumes:
- configMap:
    name: config
  name: config-volume

HOOKS:
---
# Source: python-helm/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "helm-hooks-python-helm-test-connection"
  labels:
    helm.sh/chart: python-helm-0.1.0
    app.kubernetes.io/name: python-helm
    app.kubernetes.io/instance: helm-hooks
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['helm-hooks-python-helm:8080']
  restartPolicy: Never
MANIFEST:
---
# Source: python-helm/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
 name: config
 labels:
    helm.sh/chart: python-helm-0.1.0
    app.kubernetes.io/name: python-helm
    app.kubernetes.io/instance: helm-hooks
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
data:
 config.json: |-
    {
      "app_name": "my_web_app",
      "environment": "production",
      "HTTP_PORT": 8080,
      "GRPC_PORT": 20051,
      "LOG_FORMAT": "json",
      "METRICS": "solomon",
      "kafka": {
        "host": "localhost",
        "port": 80
      },
      "logging": {
        "level": "debug",
        "file_path": "/log/my_app.log"
      },
      "api_keys": {
        "yandex_maps": "YOUR_API_KEY"
      }
    }
---
# Source: python-helm/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: helm-hooks-python-helm
  labels:
    helm.sh/chart: python-helm-0.1.0
    app.kubernetes.io/name: python-helm
    app.kubernetes.io/instance: helm-hooks
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 8080
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: python-helm
    app.kubernetes.io/instance: helm-hooks
---
# Source: python-helm/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: helm-hooks-python-helm
  labels:
    helm.sh/chart: python-helm-0.1.0
    app.kubernetes.io/name: python-helm
    app.kubernetes.io/instance: helm-hooks
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  serviceName: "helm-hooks-python-helm"
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: python-helm
      app.kubernetes.io/instance: helm-hooks
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/agent-inject-secret-config.txt: internal/data/data/config
        vault.hashicorp.com/role: internal-app
      labels:
        helm.sh/chart: python-helm-0.1.0
        app.kubernetes.io/name: python-helm
        app.kubernetes.io/instance: helm-hooks
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: internal-app
      securityContext:
        {}
      containers:
        - name: python-helm
          securityContext:
            {}
          image: "rmoll/moscow_time_web_app:latest"
          imagePullPolicy: Always
          env:
          - name: MY_PASSWORD
            valueFrom:
              secretKeyRef:
                name: credentials
                key: password
          envFrom:
            - configMapRef:
                name: config
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 128Mi
          volumeMounts:
            - mountPath: /app/config.json
              name: config-volume
              subPath: config.json
      volumes:
        - configMap:
            name: config
          name: config-volume
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi

NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services helm-hooks-python-helm)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
```

## Task 2
```text
❯ kubectl get pods,sts,svc,pvc

NAME                                       READY   STATUS    RESTARTS   AGE
pod/helm-hooks-python-helm-0               2/2     Running   0          34s
pod/helm-hooks-python-helm-1               2/2     Running   0          24s
pod/helm-hooks-python-helm-2               2/2     Running   0          12s
pod/vault-0                                1/1     Running   0          9h
pod/vault-agent-injector-dbfc5cd77-rg75p   1/1     Running   0          9h

NAME                                      READY   AGE
statefulset.apps/helm-hooks-python-helm   3/3     76s
statefulset.apps/vault                    1/1     9h

NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/helm-hooks-python-helm     NodePort    10.99.181.179    <none>        8080:31777/TCP      76s
service/kubernetes                 ClusterIP   10.96.0.1        <none>        443/TCP             28d
service/vault                      ClusterIP   10.109.239.224   <none>        8200/TCP,8201/TCP   9h
service/vault-agent-injector-svc   ClusterIP   10.108.162.209   <none>        443/TCP             9h
service/vault-internal             ClusterIP   None             <none>        8200/TCP,8201/TCP   9h

NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/data-helm-hooks-python-helm-0   Bound    pvc-33d8a01a-983f-4e1f-88b2-eef240442948   1Gi        RWO            standard       14m
persistentvolumeclaim/data-helm-hooks-python-helm-1   Bound    pvc-a91ae8d2-3336-4b48-8356-bcf1438c37e7   1Gi        RWO            standard       2m5s
persistentvolumeclaim/data-helm-hooks-python-helm-2   Bound    pvc-47c9c1a9-b00d-4bf5-b9c1-6d46cacca9ab   1Gi        RWO            standard       12s
```

---

```text
minikube service helm-hooks-python-helm
```

There are three main routes:

1. '/': This is the root path which shows the current time in Moscow. Each time a user accesses this route, the REQUEST_COUNT increments to track the total number of requests to the web application, and the REQUEST_TIME gauge is updated to display the current time in Moscow in the human-friendly format. The counter is also written to a file named 'visits' in the 'data' directory.

2. '/metrics': This route is used for serving the metrics that the application generates. It returns a plaintext representation of the metrics, adhering to the format of the Prometheus exposition format.

3. '/visits': Last, your application has a route that displays the total number of visits to the root path. This count is read from the file named 'visits' in the 'data' directory.

---

```text
❯ kubectl exec pod/helm-hooks-python-helm-0 -- cat /app/data/visits
Defaulted container "python-helm" out of: python-helm, vault-agent, vault-agent-init (init)
20.0%        
                                                                                                                                                                   
❯ kubectl exec pod/helm-hooks-python-helm-1 -- cat /app/data/visits
Defaulted container "python-helm" out of: python-helm, vault-agent, vault-agent-init (init)
27.0%     
                                                                                                                                                                      
❯ kubectl exec pod/helm-hooks-python-helm-2 -- cat /app/data/visits
Defaulted container "python-helm" out of: python-helm, vault-agent, vault-agent-init (init)
26.0%   
```

**Difference**

This behaviour is expected and a crucial aspect of how StatefulSet works. Each pod in a StatefulSet has its own network identity and attached storage, making it possible for each pod to maintain its own state information. 

In my case, each request to the application increments the counter for that pod only, which is why the counts differ. It's also important to remember these counts can vary based on the load balancing and service routing rules when user requests are forwarded to the pods.

In a stateless application (typically deployed with Deployment), all pods would share the same count since they share the same backing storage or database. In a stateful application (deployed with StatefulSet), the count would vary from pod to pod, akin to what you're seeing here.

---

**Ordering:** 
Applications often need a set number of pods running in a certain order. For example, in a database application that runs with 3 replicas, it often needs the first pod (i.e., the pod with index 0) to be running and ready, before the second pod can be deployed. This is needed to make sure that the second pod can sync from the first pod to keep the database consistent. Explain if your application needs such ordering guarantees.

---

**Parallel Operations:**
StatefulSet provides ordering guarantees which usually means a step-wise start and termination of pods. However, if your stateful application doesn't need these ordering guarantees, you can run pods in parallel. Add podManagementPolicy: "Parallel" to your StatefulSet spec in your yaml file. With this setting, the StatefulSet controller will create and delete all pods in parallel.

## Bonus Task

n Kubernetes, update strategies determine the approach to deploying updates. Specifically for a StatefulSet, there are two update strategies: RollingUpdate and OnDelete.

1. RollingUpdate: This is the default strategy for StatefulSets. If a StatefulSet’s .spec.updateStrategy field is uninitialized or set to RollingUpdate, the StatefulSet controller will create pods in increasing order (from smallest ordinal to highest). It will also delete and recreate each pod of the StatefulSet, in a reverse order (from the highest to the smallest ordinal), while applying any updates. This means updates are gradually rolled out across the entire StatefulSet, ensuring minimal disruption as no more than one pod is unavailable during the update process. However, before creating a replacement pod with an updated configuration, the StatefulSet controller will wait for its ordinal predecessor to be completely shut down and deleted.

2. OnDelete: If a StatefulSet’s .spec.updateStrategy is set to OnDelete, the system will not automatically update the Pods in the StatefulSet whenever the StatefulSet’s pod template is updated. Instead, the Pods in the StatefulSet will only be updated when they are manually deleted by the user. This gives the user finer control over the update process, as they can manually delete pods and monitor the deployment of new pods.

The choice between update strategies depends on the needs of your workload. If you require more control over when and how updates occur OnDelete would be the preferable strategy, but if you want the system to handle updates automatically, making sure your service stays available during the update, then RollingUpdate makes more sense.
